{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oP1uun77cIh"
   },
   "source": [
    "# Spell Checker\n",
    "\n",
    "This task will involve in the creation of a spellchecking system and an evaluation of its performance. \n",
    "\n",
    "The training labelled data is in  `holbrook.txt`\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "Here we are not treating these as separate words instead we treat them like a single token.\n",
    "\n",
    "Here we are using NLTK Library functions to build our spell checker model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIVCSJV-7kDs"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "The first task is to build a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RznCZ1mw7mfk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], 'indexes': [9]}\n"
     ]
    }
   ],
   "source": [
    "def parser(file):\n",
    "    lines = open(file).readlines()\n",
    "    data = []\n",
    "    # Write your code here\n",
    "\n",
    "    for line in lines:\n",
    "        #print(line)\n",
    "        indexes=[]\n",
    "        tokens = nltk.word_tokenize(line) # Tokenize each sentence\n",
    "        #print(tokens)\n",
    "        original = tokens\n",
    "        corrected = tokens\n",
    "        #Split the data by the '|' separator to get the Original and Corrected Sentences\n",
    "        for token in tokens:\n",
    "            if(\"|\" in token):\n",
    "                index_ind = tokens.index(token)\n",
    "                word_orig = tokens[tokens.index(token)].split('|')[0]\n",
    "                word_corrected = tokens[tokens.index(token)].split('|')[1].replace(\"_\",\" \")\n",
    "                original = [word_orig if word==tokens[tokens.index(token)] else word for word in original]\n",
    "                corrected = [word_corrected if word==tokens[tokens.index(token)] else word for word in corrected]\n",
    "                indexes.append(index_ind)\n",
    "        data_local = {\n",
    "            'original':original,\n",
    "            'corrected':corrected,\n",
    "            'indexes': indexes\n",
    "        }\n",
    "        data.append(data_local)\n",
    "    return data\n",
    "    \n",
    "data = parser(\"holbrook.txt\")\n",
    "print(parser(\"holbrook.txt\")[2])\n",
    "\n",
    "assert(parser(\"holbrook.txt\")[2] == {\n",
    "    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "    'indexes': [9]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRSX4I0H7pSC"
   },
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Kt9aR2Gy7p1C"
   },
   "outputs": [],
   "source": [
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm5JL7cH7sLK"
   },
   "source": [
    "## **Task 2 : Implement Unigram Model** : \n",
    "This task will calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ge0uHS-7uEK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "0.003989361702127659\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def unigram(word):\n",
    "    # Write your code here.\n",
    "    word = word.lower()\n",
    "    words_corrected =[]\n",
    "    for data_ind in train:\n",
    "        words_corrected.append([element.lower() for element in data_ind['corrected']])\n",
    "    word_corrected_list = [item for sublist in words_corrected for item in sublist]\n",
    "    return Counter(word_corrected_list)[word]\n",
    "    \n",
    "\n",
    "def prob(word):\n",
    "    # Write your code here.\n",
    "    words_corrected =[]\n",
    "    word = word.lower()\n",
    "    for data_ind in train:\n",
    "        words_corrected.append([element.lower() for element in data_ind['corrected']])\n",
    "    word_corrected_list = [item for sublist in words_corrected for item in sublist]\n",
    "    return unigram(word)/sum(Counter(word_corrected_list).values())  # Round Upto 6 digits\n",
    "\n",
    "# Test your code with the following\n",
    "assert(unigram(\"me\")==87)\n",
    "print(unigram(\"me\"))\n",
    "print(prob(\"me\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8r8QYj78GPK"
   },
   "source": [
    "## **Task 3** : \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SV9Mu8P38IQE",
    "outputId": "9f29e22b-0f8b-4b92-9d5f-fcde3efec970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hm46Lbiz8K8M"
   },
   "source": [
    "Build a function that calculates all words with *minimal* edit distance to the misspelled word. Steps are as follows:\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoilAmFW8PCb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mine', 'mind']\n"
     ]
    }
   ],
   "source": [
    "# Function to get the unique from a list preserving the order\n",
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def get_candidates(token):\n",
    "    # Write your code here.\n",
    "    words_corrected =[]\n",
    "    for data_ind in train:\n",
    "        words_corrected.append([element for element in data_ind['corrected']])\n",
    "    word_corrected_list = [item for sublist in words_corrected for item in sublist]\n",
    "    # Get the unique list of words\n",
    "    word_corrected_list_unique = unique(word_corrected_list) \n",
    "    # Get the distance w.r.t the first token\n",
    "    dist =edit_distance(token,word_corrected_list_unique[0])\n",
    "    min_dist_word =[]\n",
    "    for word in word_corrected_list_unique:\n",
    "        # Compare the distance with the subsequent tokens and update the list accordingly\n",
    "        if(edit_distance(token,word) < dist):\n",
    "            dist = edit_distance(token,word)\n",
    "            min_dist_word.clear()\n",
    "            min_dist_word.append(word)\n",
    "        elif(edit_distance(token,word) == dist):\n",
    "            min_dist_word.append(word)\n",
    "    return min_dist_word\n",
    "        \n",
    "# Test your code as follows\n",
    "assert get_candidates(\"minde\") == ['mine', 'mind']\n",
    "print(get_candidates(\"minde\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGY-eCkN8TIM"
   },
   "source": [
    "## Task 4 :\n",
    "\n",
    "Create a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dIGKE4_P8WGP"
   },
   "outputs": [],
   "source": [
    "def correct(sentence):\n",
    "    # Write your code here\n",
    "    sentence_corrected = []\n",
    "    words_corrected =[]\n",
    "    for data_ind in train:\n",
    "        words_corrected.append([element for element in data_ind['corrected']])\n",
    "    word_corrected_list = [item for sublist in words_corrected for item in sublist]\n",
    "    # Get the list of unique tokens\n",
    "    word_corrected_list_unique = list(set(word_corrected_list))\n",
    "    for word in sentence:\n",
    "        #If the word is present in the unique corpus directly return the word\n",
    "        if(word in word_corrected_list_unique):\n",
    "            sentence_corrected.append(word)\n",
    "        else:\n",
    "            # Case when get_candidates returns multiple words\n",
    "            if(len(get_candidates(word)) >1 ):\n",
    "                #Calculate the probablities of all the words\n",
    "                prob_list = list(map(prob,get_candidates(word)))\n",
    "                #Get the word with the maximum probablity\n",
    "                word_correct = get_candidates(word)[prob_list.index(max(prob_list))]\n",
    "                sentence_corrected.append(word_correct)\n",
    "            # Case when get_candidates returns a single word\n",
    "            else:\n",
    "                sentence_corrected.append(get_candidates(word)[0])\n",
    "    return sentence_corrected\n",
    "\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oG7jC6au8kka"
   },
   "source": [
    "## **Task 5** :Accuracy Calculation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applied Methodology \n",
    "1. Pass the Incorrect Corpus from the Training Set to the Correct Method\n",
    "2. Compare the Corrected Output with the Original Corrected Data \n",
    "3. Accuracy calculated as = Sum(All the words from the Generated output matches with the Original Corrected Data) / Sum(All words in the Original Corrected Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HSXTQypR8mdR",
    "outputId": "853813e4-d71b-42a7-8e96-68d038457628"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def accuracy(test,correct):\n",
    "    # Write your code here\n",
    "    total_count = 0\n",
    "    corrected_count = 0\n",
    "    for data_ind in test:\n",
    "        #print(data_ind['corrected'])\n",
    "        #print(data_ind['original'])\n",
    "        index = len(data_ind['original'])\n",
    "        # Pass each Original sentence through Correct Method to get the corrected sentence\n",
    "        corrected_list = correct(data_ind['original'])\n",
    "        #print(corrected_list)\n",
    "        i = 0\n",
    "        while(i<index):\n",
    "            #Compare each word of the Generated Output with the Corrected List\n",
    "            if(corrected_list[i] == data_ind['corrected'][i]):\n",
    "                corrected_count += 1\n",
    "                total_count += 1\n",
    "            else:\n",
    "                total_count += 1\n",
    "            i +=1\n",
    "    return round(corrected_count/total_count, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 81.162 percent\n",
      "Time Elapsed: 96.22455143928528 secs\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy and the total execution time\n",
    "xtime = time.time()\n",
    "print(\"Accuracy : \" + str(accuracy(test,correct)*100) + \" percent\")\n",
    "ytime = time.time()\n",
    "print(\"Time Elapsed: \" + str(ytime-xtime) + \" secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9b-r2JzD8_Zh"
   },
   "source": [
    "## **Task 6 :**\n",
    "\n",
    "This task is to modify the Algorithm developed to improve the accuracy of the Spell Checker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrective Approach\n",
    "\n",
    "To improve the performance of the classifier I have gone through the following options :\n",
    "1. <b> Add One Smoothing </b> - This adds up one to every word count so that it will not make the probablity as zero, but in our classifier we are using get_candidate method which will always return a value so add one smoothing will not improve the performance.\n",
    "2. <b> N - Gram Approximation </b> - in place of using unigram this approach will use n-grams i.e. considering n-1 previous tokens to calculate the probablities and passing the n-gram tokens to get_candidates to get the nearest match, but as the corpus is too small using bigrams or trigrams will not improve the performance\n",
    "3. <b> Parts Of Speech Tagging </b> - This method tags the corresponding Part Of Speech to the words of a sentence. As the classifier is trying to predict word by word its not considering the contextual data. So incorporate the context of words in the sentence we can consider the POS tag of the word in the sentence and ignore the Proper Nouns, Proper Pronouns, Cardinal Numbers while passing the sentence through classifiers. We will consider this as an improvement approach.\n",
    "4. <b> Stemming </b> - Stemming will return the root of a word. As the classifier is only predicting on the basis of the minimal distance from the words present in the train corpus it may change the word which is spelled correctly and is used in correct context just because the word is not present in the Training Corrected Corpus. So we can ignore those words whose stem is in the dictionary it will prevent some of the changes mentioned previously. <br/><br/>\n",
    "So as per the options mentioned above I followed the following additional steps - \n",
    "\n",
    "    * Pass the sentence through the NLTK Parts Of Speech Tagging \n",
    "    * Identify the Proper noun, singular (NNP) or Proper noun, plural (NNPS) or Cardinal number (CD)\n",
    "    * Ignores the words with the NNP, NNPS, CD while passing the words through the classifier\n",
    "    * Check the stems of the words in the NLTK Dictionary, if the root is present in the dictionary ignore the words from the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_modified(sentence):\n",
    "    sentence_corrected = []\n",
    "    words_corrected = []\n",
    "    for data_ind in train:\n",
    "        words_corrected.append([element for element in data_ind['corrected']])\n",
    "    word_corrected_list = [item for sublist in words_corrected for item in sublist]\n",
    "    word_corrected_list_unique = list(set(word_corrected_list))\n",
    "    sentence = pos_tag(sentence)\n",
    "    stemmer = PorterStemmer()\n",
    "    for word in sentence:\n",
    "        if(word[0] in word_corrected_list_unique):\n",
    "            sentence_corrected.append(word[0])\n",
    "        #elif(word[1] in ('CD','NNP','.', 'PRP','PRP$') ):\n",
    "        elif(word[1] in ('CD','NNP','.') ):\n",
    "            sentence_corrected.append(word[0])\n",
    "        elif(stemmer.stem(word[0].lower()) in words.words()):\n",
    "            sentence_corrected.append(word[0])\n",
    "        else:\n",
    "            candidate_list = get_candidates(word[0])\n",
    "            if(len(candidate_list) >1 ):\n",
    "                prob_list = list(map(prob,candidate_list))\n",
    "                word_correct = candidate_list[prob_list.index(max(prob_list))]\n",
    "                sentence_corrected.append(word_correct)\n",
    "            else:\n",
    "                sentence_corrected.append(candidate_list[0])\n",
    "    return sentence_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLzaC6D28sK9"
   },
   "source": [
    "## **Task 7: Test the New Method :**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw6PzwWn7iEo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 89.877 percent\n",
      "Time Elapsed: 25.87539529800415 secs\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy and the total execution time\n",
    "xtime = time.time()\n",
    "print(\"Accuracy : \" + str(accuracy(test,correct_modified)*100) + \" percent\")\n",
    "ytime = time.time()\n",
    "print(\"Time Elapsed: \" + str(ytime-xtime) + \" secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So we have found after using the POS Tagging and Stemming the accuracy of the classifier has improved from 81% to 89% and the total execution time has also decreased from 93 sec to 25 sec"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP 18/19 Assignment 1",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
